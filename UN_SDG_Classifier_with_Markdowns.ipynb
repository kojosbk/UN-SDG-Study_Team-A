{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Sustainable Development Goals (SDGs) Classifier**\n",
    "\n",
    "![](https://www.idsa.in/system/files/sdg-un-banner.jpg)\n",
    "\n",
    "# <a id=\"contents\">Table of Contents</a><br>\n",
    "1. [**Introduction**](#introduction)\n",
    "  > 1.1 [**Problem Statement**](#problem_statement) <br>\n",
    "    1.2 [**Project Objectives**](#project_objective) <br>\n",
    "    1.3 [**Definition of Data Features**](#data)<br>\n",
    "    1.4 [**Importing Libraries**](#importing_libraries)<br>\n",
    "    1.5 [**Loading Data**](#loading_data) <br>\n",
    "    1.6 [**Initialize Comet**](#comet) <br>\n",
    "    \n",
    "2. [**Data Preprocessing**](#preprocessing)\n",
    ">   2.1 [**Cumulative Probability Over Agreement Scores**](#cumulative)<br>\n",
    "    2.2 [**Cleaning the data**](#removing_links)<br>\n",
    "\n",
    "\n",
    "3. [**Exploratory Data Analysis**](#EDA) <br>\n",
    ">   3.1 [**Visualisation**](#visualisation)<br>\n",
    "    3.2 [**Countries Involved in Sustainable Development Goals (SDG)**](#countries)<br>\n",
    "    3.3 [**Text Analysis**](#text_analysis)<br>\n",
    "    3.4 [**Class Balance**](#class_balance)<br>\n",
    "    3.5 [**Feature Engineering**](#feature_engineering)<br>\n",
    "   \n",
    "4. [**Classification Models**](#model) <br>\n",
    ">   4.1 [**Defining the Independent (X) and Dependent Variables (y)**](#def_variables) <br> \n",
    "    4.2 [**Model Preprocessing**](#modelpreprocessing) <br>\n",
    "    4.3 [**Building Models**](#buildingmodels) <br>\n",
    "      >>4.3.1 [**Logistic Regression Classifier**](#lrc) <br>\n",
    "      4.3.2 [**Decision Tree Classifier**](#dtc) <br>\n",
    "      4.3.3 [**Naive Bayes Classifier**](#nbc) <br> \n",
    "      4.3.4 [**K-Neighbors Classifier**](#knc) <br>\n",
    "      \n",
    "  >   4.4 [**Model Performance**](#modelperformance) <br>\n",
    "        \n",
    "5. [**Conclusion and Recommendation**](#Conclusion/Recommendation) <br>\n",
    "   > 5.1 [**Conclusion**](#Conclusion) <br>\n",
    "     5.2 [**Recommendation**](#Recommendation) <br><br>\n",
    "\n",
    "6. [**References**](#reference) <br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 <a id=\"introduction\"><strong>INTRODUCTION</strong></a>\n",
    "[Table of Contents](#contents)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Sustainable Development Goals (SDGs) were established in 2015 as a blueprint for peace and prosperity for people and the planet, now and into the future. The SDGs must be monitored in order to gauge development and challenges to achieving such common objectives. Teams from the United Nations evaluate streams of SDG-related papers created by governments, academia, business, and public bodies to determine how well each SDG is progressing.\n",
    "Although UNEP has experts in many domains that can help in evaluating streams of SDG-related papers, connections to the SDGs outside of their areas of expertise may be missed.\n",
    "so for that reason we , members of team A have been tasked to build an NLP module that would help identify SDGs based on articles fed to the module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 1.1 <a id=\"problem_statement\"><strong>Problem Statement</strong></a>\n",
    "[Table of Contents](#contents)<br>\n",
    "\n",
    ">* Nation teams use documents produced by governments, academia, private and public entities in the monitoring of SDGs.<br>\n",
    ">* It is a time-consuming exercise for Experts to read the documents and identify which SDGs are mentioned, or are related to it.<br> \n",
    ">* UNEP has experts in many fields, links to the SDGs that are outside their expertise may be overlooked.<br>\n",
    ">* It is essential to assess progress and obstacles to realise such shared goals . "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 1.2 <a id=\"project_objective\"><strong>Project Objectives</strong></a>\n",
    "[Table of Contents](#contents)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">* Clean the dataset so that it may be utilized for model development.\n",
    ">* Create a variety of models to identify the various SDGs.\n",
    ">* Using the provided Test Data, assess the model's accuracy in making predictions.\n",
    ">* Pick the best model for categorizing SDG articles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 <a id=\"data\"><strong>Definition of Data Features</strong></a>\n",
    "[Table of Contents](#contents)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I. Data Source\n",
    "The Sustainable Development Goals (SDGs) Community Dataset (OSDG-CD) is the end product of hundreds of volunteers' efforts on the OSDG Community Platform to advance our understanding of the SDGs (OSDG-CP). It includes hundreds of text snippets that community volunteers labeled with regard to the SDGs.\n",
    "\n",
    "#### I. Goal Description\n",
    "1. End poverty in all its forms everywhere\n",
    "2. End hunger, achieve food security and improved nutrition and promote sustainable agriculture\n",
    "3. Ensure healthy lives and promote well-being for all at all ages\n",
    "4. Ensure inclusive and equitable quality education and promote lifelong learning opportunities for all\n",
    "5. Achieve gender equality and empower all women and girls\n",
    "6. Ensure availability and sustainable management of water and sanitation for all\n",
    "7. Ensure access to affordable, reliable, sustainable and modern energy for all\n",
    "8. Promote sustained, inclusive and sustainable economic growth, full and productive employment and decent work for all\n",
    "9. Build resilient infrastructure, promote inclusive and sustainable industrialization and foster innovation\n",
    "10. Reduce inequality within and among countries\n",
    "11. Make cities and human settlements inclusive, safe, resilient and sustainable\n",
    "12. Ensure sustainable consumption and production patterns\n",
    "13. Take urgent action to combat Sustainable Development Goals (SDG) and its impacts\n",
    "14. Conserve and sustainably use the oceans, seas and marine resources for sustainable development\n",
    "15. Protect, restore and promote sustainable use of terrestrial ecosystems, sustainably manage forests, combat desertification, and halt and reverse land degradation and halt biodiversity loss\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### II. Column Definitions\n",
    "The OSDG-CD dataset is provided in a .csv format. It is a flat tabular dataset that contains the following columns:\n",
    "\n",
    ">* doi : Digital Object Identifier of the original document\n",
    ">* text_id : unique text identifier;\n",
    ">* text : text excerpt from the document;\n",
    ">* sdg : the SDG the text is validated against;\n",
    ">* labels_negative : the number of volunteers who rejected the suggested SDG label;\n",
    ">* labels_positive : the number of volunteers who accepted the suggested SDG label;\n",
    ">* agreement : agreement score based on the formula described <a href=\"https://github.com/osdg-ai/osdg-data\"> Here</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 1.4  <a id=\"importing_libraries\"><strong>Importing Libraries</strong></a>\n",
    "[Table of Contents](#contents)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pycountry'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_1304/144251311.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mpycountry\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;31m# imports for data visualisation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pycountry'"
     ]
    }
   ],
   "source": [
    "# importing basic and fundamental libaries\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import pandas as pd\n",
    "import string\n",
    "import re\n",
    "import pickle\n",
    "import pycountry\n",
    "\n",
    "# imports for data visualisation\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "from textblob import TextBlob\n",
    "from nltk.probability import FreqDist\n",
    "from wordcloud import WordCloud, ImageColorGenerator #Pip install wordcloud\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# libraries for NLP\n",
    "import nltk\n",
    "from nltk import SnowballStemmer, PorterStemmer, LancasterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "# from sklearn.feature_extraction.text import TfitrainVectorizer\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "\n",
    "# importing libraries for utility from sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.utils import resample\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_validate\n",
    "# from sklearn.feature_extraction.text import TfitrainTransformer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# importing libraries for modelling form sklearn\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "# imports for other libraires\n",
    "import pickle\n",
    "import en_core_web_sm\n",
    "import spacy\n",
    "from collections import Counter\n",
    "from textwrap import wrap\n",
    "\n",
    "# libraries to check model metrics from sklearn\n",
    "from sklearn.metrics import confusion_matrix, f1_score, classification_report,accuracy_score, recall_score, precision_score\n",
    "\n",
    "# Enable interactive visualisations in Jupyter\n",
    "from ipywidgets import interact, interact_manual, widgets\n",
    "import plotly.express as px\n",
    "\n",
    "# Suppressing unnwarranted warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 <a id=\"loading_data\"><strong>Loading Data</strong></a>\n",
    "To start importing our data, we make sure the raw data and notebook file are both in the same folder on our local system. Using the code below, we'll load the train data sets into our notebook. If the files aren't in the same folder, we'll have to point to the directory on our machine . To check that the data has loaded correctly, it is a good practice to call up the loaded data after it has been loaded.\n",
    "The information would be saved in two different data frames, one for training and testing and the other for our EDA.\n",
    "\n",
    "[Table of Contents](#contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the train & test data sets\n",
    "train = pd.read_csv('data/train.csv')\n",
    "test = pd.read_csv('data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Viewing Imported Data\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=three2></a>\n",
    "\n",
    "#### 1.5.2 <strong>Check the \"Shape\" of the data-sets</strong></a>\n",
    "As demonstrated by the shape of both datasets, the data has been separated into two sets. The form also shows that the training data set has eight columns, but the test data set has seven . Our model is designed to forecast the column that is not present in the test set. We can look for that specific item by looking for the missing entity (Column) in the test data set. After looking at both datasets, the column may be identified as the SDG column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking the shape of the data sets\n",
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=four1></a>\n",
    "\n",
    "#### 1.5.3 <strong>Dataset summary</strong></a>\n",
    "\n",
    "It is important to identify the columns that have null entries as null values can affect the performance of our model. The \"isnull\" function shows the number of null values that are contained in each column of the dataset. This data set is relatively clean \n",
    "Pandas dataframe.info() function is used to get a concise summary of the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Summary(train):\n",
    "    i = train.info()\n",
    "    print (\"NUL Values\")\n",
    "    n = train.isna().sum()\n",
    "    return i,n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Summary(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6 <a id=\"comet\"><strong>Initialize Comet</strong></a>\n",
    "[Table of Contents](#contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing comet for tracking\n",
    "from comet_ml import Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the API key (saved as environment variable)\n",
    "#initialize experiment for Comet\n",
    "experiment = Experiment(\n",
    "    api_key=\"bEFY9Hn1QccermEDT6aTyQMOA\",\n",
    "    project_name=\"sustainable-development-goals-sdgs-classifier\",\n",
    "    workspace=\"kojosbk\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> The above code is the connection between this __notebook__ and the workspace on __comet__ which helps to record our experiment<br>\n",
    ">> *The comet account used for this project belongs to a member of the team*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 <a id=\"preprocessing\"><strong>DATA PREPROCESSING</strong></a>\n",
    "[Table of Contents](#contents)\n",
    "\n",
    "In this phase, we'll look into  the data, as well as filter out the most relevant information..\n",
    " \n",
    "## 2.1 <a id=\"cumulative\"><strong>Cumulative Probability Over Agreement Scores</strong></a>\n",
    "[Table of Contents](#contents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating cumulative probability over agreement scores\n",
    "train_lambda = train['agreement'].value_counts(normalize = True).sort_index().cumsum().to_frame(name = 'p_sum')\n",
    "train_lambda.reset_index(inplace = True)\n",
    "train_lambda.rename({'index': 'agreement'}, axis = 1, inplace = True)\n",
    "\n",
    "print('Shape:', train_lambda.shape)\n",
    "display(train_lambda.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization of the agreement scores\n",
    "fig = px.line(\n",
    "    data_frame = train_lambda,\n",
    "    x = 'agreement',\n",
    "    y = 'p_sum',\n",
    "    markers = True,\n",
    "    labels = {\n",
    "        'agreement': 'Agreement Score',\n",
    "        'p_sum': 'Cumulative Probrability'\n",
    "    },\n",
    "    color_discrete_sequence = ['#1f77b4'],\n",
    "    title = 'Figure 1. Cumulative Distribution Function of the Agreement Score'\n",
    ")\n",
    "\n",
    "fig.update_traces(hovertemplate = 'Agreement score: %{x:.2f}<br>Cumulative probability: %{y:.2f}')\n",
    "fig.update_layout(\n",
    "    xaxis = {'dtick': 0.1},\n",
    "    yaxis = {'dtick': 0.25}\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keeping only the texts whose suggested sdg labels is accepted and the agreement score is at least .94\n",
    "def sdg_selector(train):\n",
    "    print('Shape before:', train.shape)\n",
    "    train = train.query('agreement >= .94 and labels_positive > labels_negative').copy()\n",
    "    print('Shape after :', train.shape)\n",
    "    \n",
    "#appling to the train data\n",
    "sdg_selector(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a dataframe for visualisation of the Distribution of Texts (Agreement >.94) over SDGs\n",
    "train_lambda = train.groupby('sdg', as_index = False).agg(count = ('text_id', 'count'))\n",
    "train_lambda['share'] = train_lambda['count'].divide(train_lambda['count'].sum()).multiply(100)\n",
    "print('Shape:', train_lambda.shape)\n",
    "display(train_lambda.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ploting the bar graph of the Distribution of Texts (Agreement >.6) over SDGs\n",
    "fig = px.bar(\n",
    "    data_frame = train_lambda,\n",
    "    x = 'sdg',\n",
    "    y = 'count',\n",
    "    custom_data = ['share'],\n",
    "    labels = {\n",
    "        'sdg': 'SDG',\n",
    "        'count': 'Count'\n",
    "    },\n",
    "    color_discrete_sequence = ['#1f77b4'],\n",
    "    title = 'Figure 2. Distribution of Texts (Agreement >.6) over SDGs'\n",
    ")\n",
    "\n",
    "fig.update_traces(hovertemplate = 'SDG %{x}<br>Count: %{y}<br>Share: %{customdata:.2f}%')\n",
    "fig.update_layout(xaxis = {'type': 'category'})\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Due to the presence of a non numerical column in our datasets, some preprocessing processes must be performed, including:\n",
    "\n",
    "\n",
    ">* letter casing :Converting all letters to upper case or lower case is called letter casing.\n",
    ">* Tokenization :Tokenizing refers to the process of converting texts to tokens. Words separated by spaces in a text are referred to as tokens.\n",
    ">* Noise removal: Unwanted characters such as HTML tags, punctuation marks, special characters, white spaces, and so on are removed.\n",
    ">* Stopwords should be removed because they don't contribute anything to the machine learning model. The nltk library can specify a list of stopwords, or it can be tailored to a particular company.\n",
    ">* Lemmatization: the process of reducing a word's several forms to a single form, such as converting \"builds,\" \"building,\" or \"built\" to the lemma.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 <a id=\"removing_links\"><strong>Cleaning the data</strong></a>\n",
    "[Table of Contents](#contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We found some noises in 'text' column,and we will be doing the following to clean it: <br>\n",
    "> - Convert all text to lowercase\n",
    "> - Remove noisy entities' such as links, punctuations,numbers and extra white space.\n",
    "> - Remove contractions: Words like ain't, isn't, will have to be expanded to \"am not\" and \"is not.\"\n",
    "> - Remove non-ascii Characters\n",
    "> - Remove Stopwords\n",
    "> - Tokenization\n",
    "> - lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing links\n",
    "train['text'] = train['text'].str.replace('http\\S+|www.\\S+', '', case=False)\n",
    "train['text'] = train['text'].str.replace(r's*https?://S+(s+|$)', ' ',case=False).str.strip()\n",
    "test['text'] = test['text'].str.replace('http\\S+|www.\\S+', '', case=False)\n",
    "test['text'] = test['text'].str.replace(r's*https?://S+(s+|$)', ' ',case=False).str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function takes in pandas dataframe, removes URL hyperlinks, stopwords, punctuation noises,contractions and lemmatize the text.\n",
    "\n",
    "def preprocess(text):\n",
    "   \n",
    "    tokenizer = TreebankWordTokenizer() \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stopwords_list = stopwords.words('english')\n",
    "    point_noise = string.punctuation + '0123456789'\n",
    "    \n",
    "    cleanText = re.sub(r'@[a-zA-Z0-9\\_\\w]+', '', text)#Remove @mentions\n",
    "    cleanText = re.sub(r'#[a-zA-Z0-9]+', '', cleanText) #Remove '#' symbols\n",
    "    cleanText = re.sub(r'RT', '', cleanText)#Remove RT from text\n",
    "    #Panding Contractions\n",
    "    # specific\n",
    "    cleanText = re.sub(r\"won\\'t\", \"will not\", cleanText)\n",
    "    cleanText = re.sub(r\"can\\'t\", \"can not\", cleanText)\n",
    "    cleanText = re.sub(r\"also\", \"\", cleanText)\n",
    "    #Panding Contractions\n",
    "    # general\n",
    "    cleanText = re.sub(r\"n\\'t\", \" not\", cleanText)\n",
    "    cleanText = re.sub(r\"\\'re\", \" are\", cleanText)\n",
    "    cleanText = re.sub(r\"\\'s\", \" is\", cleanText)\n",
    "    cleanText = re.sub(r\"\\'d\", \" would\", cleanText)\n",
    "    cleanText = re.sub(r\"\\'ll\", \" will\", cleanText)\n",
    "    cleanText = re.sub(r\"\\'t\", \" not\", cleanText)\n",
    "    cleanText = re.sub(r\"\\'ve\", \" have\", cleanText)\n",
    "    cleanText = re.sub(r\"\\'m\", \" am\", cleanText)\n",
    "    cleanText = ''.join([word for word in cleanText if word not in point_noise]) #Removing punctuations and numbers.\n",
    "    cleanText = cleanText.lower() #Lowering case\n",
    "    cleanText = \"\".join(word for word in cleanText if ord(word)<128) #Removing NonAscii\n",
    "    cleanText = tokenizer.tokenize(cleanText) #Coverting each words to tokens\n",
    "    cleanText = [lemmatizer.lemmatize(word) for word in cleanText if word not in stopwords_list] #Lemmatizing and removing stopwords\n",
    "    cleanText = [word for word in cleanText if len(word) >= 2]\n",
    "    # cleanText = ' '.join(cleanText)\n",
    "    #return cleanText\n",
    "    return cleanText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['Word\\'s'] = train['text'].apply(lambda s : len(s.split(' ')))\n",
    "### Function to count number of links in dataset, it will add extra column and store information in it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#applying the preprocess function to both the test and train data\n",
    "train[\"text\"]=train[\"text\"].apply(preprocess)\n",
    "test[\"text\"]=test[\"text\"].apply(preprocess)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Viewing cleaned data\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Awesome, we have succeeded in cleaning the text. Now we can proceed to exploring the data some more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 <a id=\"EDA\"><strong>EXPLORATORY DATA ANALYSIS</strong></a>\n",
    "\n",
    " We'll use a range of strategies in this part to maximize specific insights into our dataset, uncover underlying structure, extract relevant Information, find outliers and anomalies, and establish the optimum estimation parameters. In other words, we want to go deeper into our dataset in order to learn more about its behavior!\n",
    "\n",
    "[Table of Contents](#contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 <a id=\"visualisation of the Relivant Sdg text data\"><strong>Visualisation of the Relivant Sdg text data</strong></a>\n",
    "[Table of Contents](#contents)\n",
    "\n",
    "* Separate Datframes of Text for each SDG "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"See distribution of messages per SDG : \")\n",
    "count = train.groupby(\"sdg\").count()[\"text\"].reset_index().sort_values(by=\"text\", ascending=False)\n",
    "count.style.background_gradient(cmap=\"Purples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map the target variable name to their code for better understanding\n",
    "def sdglabler(train):\n",
    "    sdgLables = {1: \"No poverty\", 2: \"Zero Hunger\", 3: \"Good Health and well-being\", 4: \"Quality Education\", 5: \"Gender equality\", 6: \"Clean water and sanitation\", 7: \"Affordable and clean energy\", 9: \"Industry, Innovation and Infrustructure\", 8: \"Decent work and economic growth\",\n",
    "                 10: \"Reduced Inequality\", 13: \"Climate Action\", 11: \"Sustainable cites and communities\", 12: \"Responsible consumption and production\", 14: \"life below water\", 15: \"Life on land\", 16: \"Peace , Justice and strong institutions\", 17: \"Partnership for the goals\"}\n",
    "    train['SDG_Labels'] = train['sdg'].map(sdgLables)\n",
    "    \n",
    "sdglabler(train)\n",
    "# Confirm the dataset\n",
    "train.head(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for a better visualisation let view by the lables\n",
    "print(\"See distribution of messages per sdg : \")\n",
    "count = train.groupby(\"SDG_Labels\").count()[\"text\"].reset_index().sort_values(by=\"text\", ascending=False)\n",
    "count.style.background_gradient(cmap=\"Purples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a bar plot of the Sdg and Text data\n",
    "fig = px.bar(count, x='SDG_Labels', y=\"text\", color='text', title='Top SDG in the Text column',\n",
    "             template='plotly_white', labels={'ngram': 'Bigram', 'count': 'text'}).update_xaxes(categoryorder='total ascending')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations\n",
    "\n",
    ">* It can be clearly seen that __Gender equality__ has the most text of all the SDGs. This means that more Documents have been written and published on gender equality, suggesting that Gender Equality is a pressing issue.\n",
    ">* This is closely followed by Quality Education and Affordable and Clean Energy.\n",
    ">* At the other end, Responsible consumption and production has the least number of text, implying that it is the least documented and researched SDG.\n",
    ">. Also, Life on land, Reduced Inequality, Industry, Innovation and Infrastructure, and Life below water all have less than 1000 text. These are all important part of the SDGs but less documents have been written on them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 <a id=\"countries\"><strong>Countries Involved in Sustainable Development Goals (SDG)</strong></a>\n",
    "[Table of Contents](#contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part of our exploration involved finding out the countries involved in Sustainable Development Goals (SDG) from our text data. To acheive that, <br>\n",
    "> - we had to extract the countries information from each texts by defining a list of countries, <br>\n",
    "> - extracting the countries from the text using __cleaning_country__ function and <br>\n",
    "> - creating a plot to visualize the frequency those countries appear in a given SDG. we did so by extracting using __country_plot__ function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we first created an sdg_map\n",
    "sdg_dict = {'No poverty': 1, 'Zero Hunger': 2, 'Good Health and well-being': 3, 'Quality Education': 4, 'Gender equality': 5, 'Clean water and sanitation': 6, 'Affordable and clean energy': 7, 'Industry, Innovation and Infrustructure': 9,\n",
    "            'Decent work and economic growth': 8, 'Reduced Inequality': 10, 'Climate Action': 13, 'Sustainable cites and communities': 11, 'Responsible consumption and production': 12, 'life below water': 14, 'Life on land': 15}\n",
    "\n",
    "train_train = train.copy()\n",
    "train_train['text'] = train_train.text.apply(lambda x: ' '.join([str(i) for i in x])) #code to change Text colum from list to string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_list = []\n",
    "for country in pycountry.countries:\n",
    "    country_list.append(country.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_list = list(map(lambda x: x.lower(),country_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning_country(text):\n",
    "    \"\"\"\n",
    "    This function helps to extract a country from each of the text and returns the country\n",
    "    \"\"\"\n",
    "    rem_punct = re.sub(r'[^a-zA-Z ]', '', text)\n",
    "    word_split = rem_punct.lower().split()\n",
    "    country_set = set(country_list)\n",
    "    country_ext = ' '.join([t for t in word_split if t in country_set])\n",
    "    return country_ext\n",
    "\n",
    "# Implementation of the text above\n",
    "train_train['country'] = train_train.text.apply(cleaning_country)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">- Great, we have been able to create a __country__ column containing countries involved in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def country_plot(sdg: dict):\n",
    "    \"\"\" \n",
    "    This function plots a bar chart of frequency of countries involved in Sustainable Development Goals (SDG) for each sdg\n",
    "    \"\"\"\n",
    "    slider = widgets.IntSlider(value=10, min=1, max=20, step=1)\n",
    "    @interact_manual\n",
    "    def selecting(Frequency=slider, SDG=sdg_dict.keys()):\n",
    "        country_sdg = train_train['country'][train_train['sdg'] == sdg_dict[SDG]]\n",
    "        train = country_sdg.value_counts().to_frame().iloc[1:Frequency]\n",
    "        fig = px.bar(train, x=train.index, y='country', labels={'index':'Country', 'country': 'Count'},title='Bar chart of countries contributing  to this SDG',)\n",
    "        fig.show()\n",
    "        \n",
    "# calling the function above to plot our visual\n",
    "country_plot(sdg_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations\n",
    "\n",
    ">* It is interesting to know the count of countries involved with each SDG."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 <a id=\"text_analysis\"><strong>Text Analysis</strong></a>\n",
    "[Table of Contents](#contents)\n",
    "\n",
    ">Separate Dataframes of Text for each SDG. \n",
    "From here, we'll delve further into our text, looking into each term and its frequency of occurrence individually."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.1 <a id=\"most_frequent_words\"><strong>Most Frequent Words</strong></a>\n",
    "[Table of Contents](#contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most frequently used words in the whole train data set\n",
    "def mostFword(train):\n",
    "    cnt = Counter()\n",
    "    for message in train['text'].values:\n",
    "        for word in message:\n",
    "            cnt[word] += 1\n",
    "            \n",
    "    return cnt.most_common(5)\n",
    "\n",
    "#Applying function\n",
    "mostFword(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = train['text']\n",
    "allwords = []\n",
    "for wordlist in words:\n",
    "    allwords += wordlist\n",
    "    \n",
    "mostcommon = FreqDist(allwords).most_common(10000)\n",
    "wordcloud = WordCloud(width=700, height=700, background_color=\n",
    "                      'black').generate(str(mostcommon))\n",
    "fig = plt.figure(figsize=(30,10), facecolor='white')\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis('off')\n",
    "plt.tight_layout(pad=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations\n",
    "\n",
    ">* The top three words are __Policy, Country and Woman__. The frequency of these words means that a lot of the same information/documents are being written by large audiences.\n",
    ">* It is no surprise that __Woman__ is one of the most frequent word considering that __SDG 5 (Gender Equality)__ has the most number of text. This further show that much work and research are being done to achieve this goal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the length of each message for each class\n",
    "train['text_length'] = train['text'].apply(lambda x: len(x))\n",
    "train.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x=train['SDG_Labels'], y=train['text_length'], data=train, width = 0.9, color = 'indigo')\n",
    "sns.set(rc={'figure.figsize':(10,5)})\n",
    "plt.ylabel('Length of the message')\n",
    "plt.xlabel('Sentiment class')\n",
    "plt.tick_params(axis='x', rotation=90)\n",
    "plt.title('Message length for each SDG class')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations\n",
    "\n",
    ">* The above display shows the __lenght__ of text for each __SDG Label__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = train[['SDG_Labels','Word\\'s']].groupby('SDG_Labels').sum()\n",
    "m_w = max_words.sort_values('Word\\'s',ascending=False).head(10)\n",
    "# x_pos = np.arange(len(bars))\n",
    "m_w.plot.bar(rot=90)\n",
    "plt.xlabel('Name')\n",
    "plt.ylabel('No. of words')\n",
    "plt.title('The SDG with the the most number of words')\n",
    "# plt.xticks(x_pos,rotation=70)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations\n",
    "\n",
    ">>* The above display shows the __SDGs__ and the __number of times__ each appear in the text.<br>\n",
    ">>* It also shows that the SDG __Gender Equality__ has the most number of words(>300000)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 <a id=\"class_balance\"><strong>Class Balance</strong></a>\n",
    "[Table of Contents](#contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5.1 <a id=\"class_balance\"><strong>Working with balanced data</strong></a>\n",
    "[Table of Contents](#contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting the class distribution\n",
    "train.sdg.value_counts().plot(kind='bar')\n",
    "plt.ylabel('Frequency')\n",
    "plt.xlabel('sdg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> From the distribution plot above, we can see that the class distribution has wide variations and it is very imbalanced. Although, most __sdg__ support the __gender equality__ which is represented by `5` while we have a fewer responsibe consumption and production which is represented by `12`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dealing with data imbalance "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To deal with data imbalance and avoid bias predictions from the models we will build later on, we will imploy the resampling (Upsampling and Downsampling) method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['text'] = train.text.apply(lambda x: ' '.join([str(i) for i in x])) #code to change from list to string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['text'] = test.text.apply(lambda x: ' '.join([str(i) for i in x])) #code to change from list to string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_size = 3438 # setting an average or fixed size for each category in 'sdg'\n",
    "dd = train.sdg.value_counts() # getting category name and their size\n",
    "appended_data = [] # creating an empty list to append all category after resampling\n",
    "\n",
    "# Creating a for-loop to resample and append to a list\n",
    "for index, size in dd.items():\n",
    "    if size < class_size: # setting condition to check if it's downsampling or otherwise\n",
    "        temp_pd = resample(train[train['sdg']==index],\n",
    "                          replace=True, # sample with replacement\n",
    "                          n_samples=class_size, # match number in majority class\n",
    "                          random_state=27)\n",
    "    else:\n",
    "        temp_pd = resample(train[train['sdg']==index],\n",
    "                          replace=False, # sample with replacement (no need to duplicate observations)\n",
    "                          n_samples=class_size, # match number in minority class\n",
    "                          random_state=27)\n",
    "# Appending each category after resampling\n",
    "    appended_data.append(temp_pd)\n",
    "\n",
    "# Creating a new dataframe and viewing\n",
    "train_train_sampled = pd.concat(appended_data, axis=0)\n",
    "print('dual-sampling:')\n",
    "display(train_train_sampled.sample(1))\n",
    "print('Class with their new resampled sizes')\n",
    "# print(train_train_sampled.sdg.value_counts())\n",
    "\n",
    "# ploting the old and new graph frequency\n",
    "ax = train.sdg.value_counts().plot(kind='bar')\n",
    "train_train_sampled.sdg.value_counts().plot(kind='bar', title='Count (target)', \n",
    "                                               ax=ax,color='orange', alpha=0.5)\n",
    "plt.xticks(range(len(dd)),dd.index)\n",
    "plt.ylabel(\"# of observations\")\n",
    "plt.legend(['original','resampled'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">>* Great, from the distribution plot above, we can clearly see that the class distribution is __balanced__ .\n",
    ">>* It contains an equal number of samples from the positive and negative class giving priority to each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Working with resampled data\n",
    "train = train_train_sampled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 <a id=\"feature_engineering\"><strong>Feature Engineering</strong></a>\n",
    "[Table of Contents](#contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5.1 Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point,we are using **CountVectorizer** function to convert the cleaned text to numerical entries in a matrix form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting each word in the dataset into features\n",
    "vectorizer = CountVectorizer(analyzer = \"word\", max_features = 8000) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> we set the maximum feature to random number 8000 by discretion and to have a faster training of our model<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transforming the cleaned text to numerical entries in a matrix form\n",
    "reviews_vect = vectorizer.fit_transform(train['text'])\n",
    "\n",
    "# converting the features to numpy array for fast fitting of our dataset\n",
    "train_data_features = reviews_vect.toarray()\n",
    "\n",
    "# checking the shape to be sure of the size of the features\n",
    "print(f'Our new vectorized features have {train_data_features.shape[0]} rows and {train_data_features.shape[1]} feaures')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the word of the all text into features using 'vectorizer' object created earlier\n",
    "reviews_vect_test = vectorizer.transform(test['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting into a numpy array\n",
    "test_data_features = reviews_vect_test.toarray() # for fast fitting of our dataset\n",
    "test_data_features.shape # checking to be sure is the same size with our 'train' dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Double checking our dataframe was modified rightly\n",
    "test.sample(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"six\"></a>\n",
    "# 4 <a id=\"model\"><strong>Classification Models</strong></a>\n",
    "[Table of Contents](#contents)\n",
    "<a class=\"anchor\" id=\"1.1\"></a>\n",
    "<a href=#cont>Back to Table of Contents</a>\n",
    "\n",
    "---\n",
    "    \n",
    "\n",
    " In this section, we'll look at different models, try to explain them, and learn about their benefits and drawbacks. |\n",
    "\n",
    "---\n",
    "\n",
    "*  LinearSVC(Support Vector Classifier)<a id='svc'></a>\n",
    "*  Naives Bayes\n",
    "*  Logistic Regression\n",
    "*  K-Neighbors Classifier\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"six\"></a>\n",
    "## 4.1 <a id=\"def_variables\"><strong>Defining the Independent (X) and Dependent Variables (y)</strong></a>\n",
    "[Table of Contents](#contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting the target and features \n",
    "y = np.array(train['sdg'])\n",
    "X = train_data_features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  * Spliting into Train and validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Here, the train data set is divided into train and validation data set. The validation set size is 20% of the total records and it will be used in model evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the target ad features into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Splitting of the dataset into train and validation dataset, so that we can train and also see the performance of the model on unseen dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 <a id=\"modelpreprocessing\"><strong>Model Preprocessing</strong></a>\n",
    "[Table of Contents](#contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### * Defining Function for Building Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function implements a machine learning moded with the use of the train_test_split concept. The function calculates various metrics to check for underfitting or overfitting and most importantly the performance of the model being executed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdgLables = {'No poverty': 1, 'Zero Hunger': 2, 'Good Health and well-being': 3, 'Quality Education': 4, 'Gender equality': 5, 'Clean water and sanitation': 6, 'Affordable and clean energy': 7, 'Industry, Innovation and Infrustructure': 9,\n",
    "            'Decent work and economic growth': 8, 'Reduced Inequality': 10, 'Climate Action': 13, 'Sustainable cites and communities': 11, 'Responsible consumption and production': 12, 'life below water': 14, 'Life on land': 15}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# careating a function to fit the model and calculate various metrics\n",
    "def modelling(model, X_train=X_train, X_test=X_test, y_train=y_train, y_test=y_test):\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    pred_train = model.predict(X_train)\n",
    "    pred_valid = model.predict(X_test)\n",
    "    \n",
    "# calculating the metrics\n",
    "    f1_train = f1_score(y_train, pred_train, average='weighted')\n",
    "    f1_test = f1_score(y_test, pred_valid,  average='weighted')\n",
    "    cm = confusion_matrix(y_test, pred_valid)\n",
    "    # sdgLables = {v: k for k, v in sdgLables.items()}\n",
    "    cr = classification_report(y_test, pred_valid, target_names=sdgLables, output_dict=True)\n",
    "    \n",
    "# creating a dataframe for 'confusion matrix' and 'classification report'\n",
    "    cm_train = pd.DataFrame(cm, index = sdgLables, columns = sdgLables)\n",
    "    cr_train = pd.DataFrame(cr)\n",
    "    \n",
    "    print('Confusion Matrix')\n",
    "    display(cm_train)\n",
    "    print('Classification Report')\n",
    "    display(cr_train)\n",
    "    \n",
    "# plotting the confusion matrix using heatmap\n",
    "    plt.figure(figsize=(10,5))\n",
    "    sns.heatmap(cm_train, annot=True)\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.ylabel('Actal Values')\n",
    "    plt.xlabel('Predicted Values')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f'Train accuracy(f1) is: {round(f1_train, 3)} and Validation accuracy(f1) is {round(f1_test, 3)}')\n",
    "    return cm_train, cr_train, cm # returning metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 <a id=\"buildingmodels\"><strong>Building Models</strong></a>\n",
    "[Table of Contents](#contents)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.1 <a id=\"lrc\"><strong>Logistic Regression Classifier</strong></a>\n",
    "[Table of Contents](#contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* __Logistics regression__ is a predictive analytics model that is used to describe data and explain the relationship between one dependent binary variable and one or more nominal, ordinal, interval or ratio-level independent variables.<br>\n",
    "* Sometimes, it can also be applied to a dependent variable that is multi-classed (ie have more than 2 variables). The model was adopted because it is a classification algorithm and suited for our use case as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# create logistic regression model instance\n",
    "lm = LogisticRegression(C=2, solver='liblinear')\n",
    "\n",
    "# fitting and predictiion by calling the 'modelling' fuction \n",
    "cm_train_logit, cr_train_logit, cm_logit = modelling(lm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the confusion matrix we can see that the model did well at predicting the `Gender equality`. Therefore, we have more correctly predicted `Gender equality` sentiment than others with an accuracy of `78%`\n",
    "\n",
    "* Generally the model is overfitting having an accuracy score of `99%` in training vs `92%` while predicting unseen dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionaries for the data we want to log on comet\n",
    "\n",
    "params = {\"random_state\": 10,\n",
    "          \"model_type\": \"lm\"\n",
    "          }\n",
    "metrics = {\"f1\": cr_train_logit.loc['f1-score', 'weighted avg'],\n",
    "           \"precision\": cr_train_logit.loc['precision', 'weighted avg'],\n",
    "           \"recall\": cr_train_logit.loc['recall', 'weighted avg']\n",
    "           }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log our parameters and results on comet\n",
    "experiment.log_parameters(params)\n",
    "experiment.log_metrics(metrics)\n",
    "experiment.log_confusion_matrix(matrix=cm_logit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.2 <a id=\"dtc\"><strong>Decision Tree Classifier</strong></a>\n",
    "[Table of Contents](#contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Decision Tree__ is a Supervised learning technique that can be used for both classification and Regression problems, but mostly it is preferred for solving Classification problems. It is a tree-structured classifier,<br>\n",
    ">- where internal nodes represent the features of a dataset,<br>\n",
    ">- branches represent the decision rules ,and<br>\n",
    ">- each leaf node represents the outcome.<br>\n",
    "In order to make prediction, the algorithm simply asks a question, and based on the answer (Yes/No), it further splits the tree into subtrees till it gets to the final outcome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inintiate Decision tree\n",
    "dt=DecisionTreeClassifier(random_state=1)\n",
    "\n",
    "#initiate a BaggingClassifier called bc consisting of 50 trees\n",
    "bc=BaggingClassifier(base_estimator=dt, n_estimators=50, random_state=1, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_train_dt, cr_train_dt, cm_dt = modelling(dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From our result, the model performed poorly on unseen data. With the train data, an accuracy score of `100%` was recorded but on the test data, we have an accuracy score of `57.4%` which depicts overfitting. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moving forward to build the rest of our model, we employed techiques like `Pipeline` and `GridsearchCV`.\n",
    "\n",
    "**Pipeline**\n",
    "\n",
    "When it comes to orchestrating the flow of data into and output from a machine learning model, a machine learning pipeline comes in handy. It is known as an end-to-end structure that orchestrates the flow of data into and output from a machine learning model is known as a machine learning pipeline (or set of multiple models). It covers the input of the raw data, the features, the outputs, the machine learning model and model parameters, and the outputs of the predictions.\n",
    "\n",
    "Here, a machine learning pipeline is employed to easy feature extraction from our text data. The pipeline employed executes the following steps in chronological order to extract these features and make predictions. They include:\n",
    "\n",
    "* Count Vectorization: The CountVectorizer tool in Scikit-learn is used to turn a group of text documents into a vector of term/token counts. Additionally, it enables text data to be pre-processed before being turned into a vector form. Its capabilities turn it into a very versatile feature representation module for text.\n",
    "&emsp;\n",
    "\n",
    "\n",
    "* TF-ID Transformer: With TF-ID transformer, a count matrix is transformed into a normalised tf or tf-itrain representation. To reduce the impact of tokens that appear often in a corpus, tf-itrain is used in place of the raw frequencies of occurrence of a token in a specific document \n",
    "\n",
    "\n",
    "\n",
    "* Model: This represents the adopted Machine Learning model\n",
    "\n",
    "\n",
    "**GridsearchCV**\n",
    "\n",
    "GridSearchCV is the process of performing hyperparameter tuning in order to determine the optimal values for a given model. The performance of a model significantly depends on the value of hyperparameters and there is no way to know in advance the best values for hyperparameters. So ideally, we need to try all possible values to know the optimal values. That is why we employ GridSearchCV to automate the tuning of hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.3 <a id=\"nbc\"><strong>Naive Bayes Classifier</strong></a>\n",
    "[Table of Contents](#contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mostly employed in Natural Language Processing, the Multinomial Naive Bayes algorithm is a probabilistic learning technique (NLP). The method, which guesses the tag of a text such as an email or newspaper article, is based on the Bayes theorem. For a given sample, it determines the probabilities of each tag, and then outputs the tag with the highest probability.\n",
    "\n",
    "The Naive Bayes classifier is a collection of many methods, all of which are based on the idea that each feature being classified is independent of every other feature. The existence or absence of one feature has no bearing on the other feature's existence or absence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the target and features into train and test\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(train['text'], train.sdg, test_size=0.33, random_state=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Here, we split our dataset again into `Train` and `Validation` set for the purpose of fitting the data into our pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Building a pipeline to be more concise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipline with naive_bayes and using the 'clean_text' from dataset to fit the model\n",
    "text_clf = Pipeline([('vect', CountVectorizer()),\n",
    "                     ('tfitrain', TfitrainTransformer()),\n",
    "                     ('clf', MultinomialNB()),])\n",
    "# cross validation\n",
    "scores = cross_validate(text_clf, train['text'], train.sdg, return_train_score=True)\n",
    "pd.DataFrame(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - The model seem to be under performing and overfitting at the same time\n",
    "> - Using __GridsearchCV__ we can find the best hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'vect__ngram_range': [(1, 1), (1, 2), (2, 2)],\n",
    "              'clf__alpha': (1e-2, 1e-3),\n",
    "             'clf__fit_prior': (True, False)}\n",
    "nb_clf = GridSearchCV(text_clf, parameters, n_jobs=-1)\n",
    "nb_clf = nb_clf.fit(x_train, y_train)\n",
    "display('The best accuracy is:',nb_clf.best_score_)\n",
    "display(nb_clf.best_params_)\n",
    "\n",
    "# predictions\n",
    "predicts = nb_clf.predict(x_valid)\n",
    "cm = confusion_matrix(y_valid,  predicts)\n",
    "pd.DataFrame(cm, index = sdgLables, columns = sdgLables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - Using our split dataset so we can train on seen data and validate our model with the unseen dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionaries for the data we want to log\n",
    "params = nb_clf.best_params_\n",
    "metrics = {\"f1\": nb_clf.best_score_}\n",
    "\n",
    "# Log our parameters and results\n",
    "experiment.log_parameters(params)\n",
    "experiment.log_metrics(metrics)\n",
    "experiment.log_confusion_matrix(matrix=cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.4 <a id=\"knc\"><strong>K-Neighbors Classifier</strong></a>\n",
    "[Table of Contents](#contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building a pipeline to be more concise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<script src=\"https://github.com/kojosbk/RentingProject/blob/main/rent_in_Ghana%20copy.ipynb\"></script>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# pipline with naive_bayes and using the 'clean_text' from dataset to fit the model\n",
    "knc_clf = Pipeline([('vect', CountVectorizer()),\n",
    "                    ('tfitrain', TfitrainTransformer()),\n",
    "                     ('clf', KNeighborsClassifier()),])\n",
    "# cross validation\n",
    "scores = cross_validate(knc_clf, train['text'], train.sdg, return_train_score=True)\n",
    "pd.DataFrame(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">- The model seem to be under performing and overfitting at the same time\n",
    ">- Using GridsearchCV we can find the best hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'vect__ngram_range': [(1, 1), (1, 2)],\n",
    "              'clf__n_neighbors': (1, 31)}\n",
    "knn_clf = GridSearchCV(knc_clf, parameters, n_jobs=-1)\n",
    "knn_clf = knn_clf.fit(x_train, y_train)\n",
    "display(f'The best accuracy is: {knn_clf.best_score_}')\n",
    "display(knn_clf.best_params_)\n",
    "\n",
    "# predictions\n",
    "predicts = knn_clf.predict(x_valid)\n",
    "cm = confusion_matrix(y_valid,  predicts)\n",
    "pd.DataFrame(cm, index = sdgLables, columns = sdgLables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionaries for the data we want to log\n",
    "\n",
    "params = knn_clf.best_params_\n",
    "metrics = {\"f1\": knn_clf.best_score_}\n",
    "\n",
    "# Log our parameters and results\n",
    "experiment.log_parameters(params)\n",
    "experiment.log_metrics(metrics)\n",
    "experiment.log_confusion_matrix(matrix=cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The above cells are used to log/commit our model to comet "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 <a id=\"modelperformance\"><strong>Model Performance</strong></a>\n",
    "[Table of Contents](#contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to evaluate how well our models have performed, we employed the following classification metrics:\n",
    "\n",
    "* **1. Accuracy**\n",
    "\n",
    "Accuracy simply measures how often the classifier correctly predicts. We can define accuracy as the ratio of the number of correct predictions and the total number of predictions.\n",
    "\n",
    "* **2. Precision**\n",
    "\n",
    "Precision explains how many of the correctly predicted cases actually turned out to be positive. Precision is useful in the cases where False Positive is a higher concern than False Negatives.\n",
    "\n",
    "* **3. Recall**\n",
    "\n",
    "Recall explains how many of the actual positive cases we were able to predict correctly with our model. It is a useful metric in cases where False Negative is of higher concern than False Positive.\n",
    "\n",
    "* **4. F1-Score**\n",
    "\n",
    "It gives a combined idea about Precision and Recall metrics.F1 Score is the harmonic mean of precision and recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting with each model of choice\n",
    "lm_preds = lm.predict(X_test) # LogisticRegression\n",
    "nb_clf_preds = nb_clf.predict(x_valid) # Naive Bayes\n",
    "knn_clf_preds = knn_clf.predict(x_valid)# K-Neighbors Classifier\n",
    "dt_preds = dt.predict(X_test) # Decision Tree Classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a map for our model and there prediction array of values\n",
    "metrics_dict = {\"LogisticRegression\":lm_preds, \"Naive Bayes\":nb_clf_preds, \"K-Neighbors Classifier\":knn_clf_preds,\n",
    "                \"Decision Tree Classifier\": dt_preds}\n",
    "metrics_list = [] #initiating a new list to store our metrics value temporarily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterating through the dictionary to calculate metrics for each model and make a dataframe off it\n",
    "for name, prediction in metrics_dict.items():\n",
    "    f1_score = metrics.f1_score(y_valid, prediction, average=\"weighted\")  \n",
    "    precision = metrics.precision_score(y_valid, prediction, average=\"weighted\")\n",
    "    recall = metrics.recall_score(y_valid, prediction, average=\"weighted\")\n",
    "    metrics_list.append([name, f1_score, precision, recall])\n",
    "perf_train = pd.DataFrame(metrics_list, columns=[\"Classifier\", \"F1_score\", \"Precision\", \"Recall\"])\n",
    "perf_train.set_index(\"Classifier\", inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# overview of the dataframe created above\n",
    "perf_train = perf_train.sort_values(by=['F1_score'], ascending=False)\n",
    "perf_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting the metrics complied \n",
    "perf_train.plot(kind=\"bar\", figsize=(15,6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the Classification metrics point to `Naive Bayes` as the better Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### predicting our `test` DataSet with the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive bayes \n",
    "# Using the fitted (train) model to predict the 'test' dataset\n",
    "pred_test = nb_clf.predict(test['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving test predictions to csv file\n",
    "output = pd.DataFrame({'id': test.id,\n",
    "                       'sdg': pred_test})\n",
    "output.to_csv('Team A1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment.end()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.0 <a id=\"Conclusion/Recommendation\"><strong>CONCLUSION AND RECOMMENDATION</strong></a>\n",
    "[Table of Contents](#contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In this project, the team developed and built a  Multi-label text classification Machine Learning Model to improve the text labelling process, which will save time for the UN and UNEP experts, and also improve the efficiency of labelling documents to related SDGs ,especially for fields that are outside the scope of the experts which may be overlooked.\n",
    "- Monitoring of SDGs is essential to access progress and obstacle to realise shared goals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 <a id=\"Conclusion\"><strong>Conclusion</strong></a>\n",
    "[Table of Contents](#contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Approximately 30% of the SDGs (Responsible Consumption and Production, life on land, Reduced Inequality, Industry, Innovation and Infrastructure, and Life below Water) have less that 1000 number of words. This means that less documents are produced on these SDGs, implies that there are obstacles to the success of these goals.\n",
    "- For this project, four classification models consisting of Linear SVC, Naives Bayes, Logistic Regression and K-Neighbors Classifier models were trial-tested"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 <a id=\"Recommendation\"><strong>Recommendation</strong></a>\n",
    "[Table of Contents](#contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The UN should consider requesting that more research and documents be done and written on these SDGs with very low number of text by Governments, academia, private and public entities in order to improve the success and progress of these goals. Because achieving high level of of success in some goals and less in other goals is not ideal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.0 <a id=\"reference\"><strong>REFERENCES</strong></a>\n",
    "[Table of Contents](#contents)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "320c1f05b41b6296d6cdeadbc8f37198b22e160db062b16d8b8cc9d95c25d782"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
