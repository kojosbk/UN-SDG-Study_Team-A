{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Sustainable Development Goals (SDGs) Classifier**\n",
    "\n",
    "![](https://www.idsa.in/system/files/sdg-un-banner.jpg)\n",
    "\n",
    "# <a id=\"contents\">Table of Contents</a><br>\n",
    "1. [**Introduction**](#introduction)\n",
    "  > 1.1 [**Problem Statement**](#problem_statement) <br>\n",
    "    1.2 [**Data Description**](#data)<br>\n",
    "    1.3 [**Importing Libraries**](#importing_libraries)<br>\n",
    "    1.4 [**Loading Data**](#loading_data) <br>\n",
    "    \n",
    "2. [**Data Preprocessing**](#preprocessing)\n",
    ">   2.1 <br>\n",
    "    2.2 <br>\n",
    "    2.3 <br>\n",
    "\n",
    "3. [**Exploratory Data Analysis**](#EDA) <br>\n",
    "   \n",
    "4. [**Modeling**](#model) <br>\n",
    "    4.1 [**Model Training**](#modeltraining) <br>\n",
    "    > 4.1.1. [**Model 1**](#model1) <br>\n",
    "     4.1.2. [**Model 2**](#model2) <br>\n",
    "     4.1.3. [**Model 3**](#model3) <br>\n",
    "     \n",
    "    4.2 [**Mertics Evaluation**](#merticsevaluation) <br>\n",
    "    \n",
    "5. [**Conclusion and Recommendation**](#Conclusion/Recommendation) <br>\n",
    "   > 5.1 [**Conclusion**](#Conclusion) <br>\n",
    "     5.2 [**Recommendation**](#Recommendation) <br><br>\n",
    "\n",
    "6. [**References**](#reference) <br>\n",
    "\n",
    "# 1.0 <a id=\"introduction\"><strong>INTRODUCTION</strong></a>\n",
    "[Table of Contents](#contents)<br>\n",
    "\n",
    "\n",
    "\n",
    "## 1.1 <a id=\"problem_statement\"><strong>Problem Statement</strong></a>\n",
    "[Table of Contents](#contents)<br>\n",
    "\n",
    "\n",
    "## 1.2 <a id=\"data\"><strong>Data Description</strong></a>\n",
    "[Table of Contents](#contents)<br>\n",
    "\n",
    "\n",
    " ## 1.3  <a id=\"importing_libraries\"><strong>Importing Libraries</strong></a>\n",
    "[Table of Contents](#contents)<br>\n",
    "\n",
    "\n",
    "## 1.4 <a id=\"loading_data\"><strong>Loading Data</strong></a>\n",
    "[Table of Contents](#contents)\n",
    "\n",
    "\n",
    "# 2.0 <a id=\"preprocessing\"><strong>DATA PREPROCESSING</strong></a>\n",
    "[Table of Contents](#contents)\n",
    "\n",
    "\n",
    "# 3.0 <a id=\"EDA\"><strong>EXPLORATORY DATA ANALYSIS</strong></a>\n",
    "[Table of Contents](#contents)\n",
    "\n",
    "\n",
    "\n",
    "# 4.0 <a id=\"model\"><strong>Classification Models</strong></a>\n",
    "[Table of Contents](#contents)\n",
    "\n",
    "\n",
    "## 4.1 <a id=\"modelpreprocessing\"><strong>Model Preprocessing</strong></a>\n",
    "[Table of Contents](#contents)\n",
    "\n",
    "\n",
    "## 4.2 <a id=\"modeltraining\"><strong>Model Training</strong></a>\n",
    "[Table of Contents](#contents)\n",
    "\n",
    "\n",
    "### 4.2.1 <a id=\"model1\"><strong>Model 1</strong></a>\n",
    "[Table of Contents](#contents)\n",
    "\n",
    "\n",
    "### 4.2.2 <a id=\"model2\"><strong>Model 2</strong></a>\n",
    "[Table of Contents](#contents)\n",
    "\n",
    "\n",
    "### 4.2.3 <a id=\"model3\"><strong>Model 3</strong></a>\n",
    "[Table of Contents](#contents)\n",
    "\n",
    "\n",
    "# 5.0 <a id=\"Conclusion/Recommendation\"><strong>CONCLUSION AND RECOMMENDATION</strong></a>\n",
    "[Table of Contents](#contents)\n",
    "\n",
    "## 5.1 <a id=\"Conclusion\"><strong>Conclusion</strong></a>\n",
    "[Table of Contents](#contents)\n",
    "\n",
    "## 5.2 <a id=\"Recommendation\"><strong>Recommendation</strong></a>\n",
    "[Table of Contents](#contents)\n",
    "\n",
    "# 6.0 <a id=\"reference\"><strong>REFERENCES</strong></a>\n",
    "[Table of Contents](#contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c072c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing comet for tracking\n",
    "from comet_ml import Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f8a77cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the API key (saved as environment variable)\n",
    "#initialize experiment for Comet\n",
    "experiment = Experiment(\n",
    "    api_key=\"bEFY9Hn1QccermEDT6aTyQMOA\",\n",
    "    project_name=\"sustainable-development-goals-sdgs-classifier\",\n",
    "    workspace=\"kojosbk\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <a id=\"one\"></a>\n",
    "## 1. <strong>INTRODUCTION</strong></a>\n",
    "<a href=#cont>Back to Table of Contents</a>\n",
    "\n",
    "---\n",
    "    \n",
    "| ⚡ Description: INTRODUCTION ⚡ |\n",
    "| :--------------------------- |\n",
    "| We will address the problem statement and objectives, as well as the classification of data aspects and a brief discussion of the Climate Change Belief Analysis in this part.|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Sustainable Development Goals (SDGs) were established in 2015 as a blueprint for peace and prosperity for people and the planet, now and into the future. The SDGs must be monitored in order to gauge development and challenges to achieving such common objectives. Teams from the United Nations evaluate streams of SDG-related papers created by governments, academia, business, and public bodies to determine how well each SDG is progressing.\n",
    "Although UNEP has experts in many domains that can help in evaluating streams of SDG-related papers, connections to the SDGs outside of their areas of expertise may be missed.\n",
    "so for that reason we , members of team A have been tasked to build an NLP module that would help identify SDGs based on articles fed to the module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 <strong>Problem Statement</strong></a>\n",
    "---\n",
    "\n",
    "to be typed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"po\"></a>\n",
    "\n",
    "### 1.2 <strong>Project Objectives</strong></a>\n",
    "---\n",
    "\n",
    "* Clean the dataset so that it may be utilized for model development.\n",
    "\n",
    "* Create a variety of models to identify the various SDGs.\n",
    "\n",
    "* Using the provided Test Data, assess the model's accuracy in making predictions.\n",
    "\n",
    "* Pick the best model for categorizing SDG articles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"dodf\"></a>\n",
    "\n",
    "### 1.3 <strong>Definition of Data Features</strong></a>\n",
    "---\n",
    "#### I. Data Source\n",
    "The Sustainable Development Goals (SDGs) Community Dataset (OSDG-CD) is the end product of hundreds of volunteers' efforts on the OSDG Community Platform to advance our understanding of the SDGs (OSDG-CP). It includes hundreds of text snippets that community volunteers labeled with regard to the SDGs.\n",
    "\n",
    "#### I. Goal Description\n",
    "1. End poverty in all its forms everywhere\n",
    "2. End hunger, achieve food security and improved nutrition and promote sustainable agriculture\n",
    "3. Ensure healthy lives and promote well-being for all at all ages\n",
    "4. Ensure inclusive and equitable quality education and promote lifelong learning opportunities for all\n",
    "5. Achieve gender equality and empower all women and girls\n",
    "6. Ensure availability and sustainable management of water and sanitation for all\n",
    "7. Ensure access to affordable, reliable, sustainable and modern energy for all\n",
    "8. Promote sustained, inclusive and sustainable economic growth, full and productive employment and decent work for all\n",
    "9. Build resilient infrastructure, promote inclusive and sustainable industrialization and foster innovation\n",
    "10. Reduce inequality within and among countries\n",
    "11. Make cities and human settlements inclusive, safe, resilient and sustainable\n",
    "12. Ensure sustainable consumption and production patterns\n",
    "13. Take urgent action to combat climate change and its impacts\n",
    "14. Conserve and sustainably use the oceans, seas and marine resources for sustainable development\n",
    "15. Protect, restore and promote sustainable use of terrestrial ecosystems, sustainably manage forests, combat desertification, and halt and reverse land degradation and halt biodiversity loss\n",
    "16. Promote peaceful and inclusive societies for sustainable development, provide access to justice for all and build effective, accountable and inclusive institutions at all levels\n",
    "17. Strengthen the means of implementation and revitalize the Global Partnership for Sustainable Development\n",
    "\n",
    "\n",
    "\n",
    "#### II. Column Definitions\n",
    "The OSDG-CD dataset is provided in a .csv format. It is a flat tabular dataset that contains the following columns:\n",
    "\n",
    "* doi : Digital Object Identifier of the original document\n",
    "* text_id : unique text identifier;\n",
    "* text : text excerpt from the document;\n",
    "* sdg : the SDG the text is validated against;\n",
    "* labels_negative : the number of volunteers who rejected the suggested SDG label;\n",
    "* labels_positive : the number of volunteers who accepted the suggested SDG label;\n",
    "* agreement : agreement score based on the formula described <a href=\"https://github.com/osdg-ai/osdg-data\"> Here</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <a id=\"two\"></a>\n",
    "## 2. <strong>Import Necessary Libraries</strong></a>\n",
    "<a href=#cont>Back to Table of Contents</a>\n",
    "\n",
    "---\n",
    "    \n",
    "| ⚡ Description: Import necessary libraries ⚡ |\n",
    "| :--------------------------- |\n",
    "| We'd be importing all of the necessary libraries for the notebook to run smoothly..|\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing basic and fundamental libaries\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import pandas as pd\n",
    "import string\n",
    "import re\n",
    "import pickle\n",
    "import pycountry\n",
    "\n",
    "# imports for data visualisation\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "from textblob import TextBlob\n",
    "from nltk.probability import FreqDist\n",
    "from wordcloud import WordCloud, ImageColorGenerator #Pip install wordcloud\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# libraries for NLP\n",
    "import nltk\n",
    "from nltk import SnowballStemmer, PorterStemmer, LancasterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "\n",
    "# importing libraries for utility from sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.utils import resample\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# importing libraries for modelling form sklearn\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "# imports for other libraires\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import en_core_web_sm\n",
    "import spacy\n",
    "# libraries to check model metrics from sklearn\n",
    "from sklearn.metrics import confusion_matrix, f1_score, classification_report,accuracy_score\n",
    "\n",
    "# Enable interactive visualisations in Jupyter\n",
    "from ipywidgets import interact, interact_manual, widgets\n",
    "import plotly.express as px\n",
    "\n",
    "# Suppressing unnwarranted warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Three\"></a>\n",
    "## 3. <strong>Loading the Data</strong></a>\n",
    "<a class=\"anchor\" id=\"1.1\"></a>\n",
    "<a href=#cont>Back to Table of Contents</a>\n",
    "\n",
    "---\n",
    "    \n",
    "| ⚡ Description: Loading the data ⚡ |\n",
    "| :--------------------------- |\n",
    "| The data from the `train` file is loaded into a DataFrame in this section.. |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the train & test data sets\n",
    "train = pd.read_csv('data/train.csv')\n",
    "test = pd.read_csv('data/test.csv')\n",
    "\n",
    "# EDA Datasets\n",
    "train_eda = pd.read_csv('data/train.csv')\n",
    "test_eda = pd.read_csv('data/test.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_eda.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=three1></a>\n",
    "\n",
    "#### 3.1 <strong>Set Pandas to enable viewing of all columns</strong></a>\n",
    "Due to the length of th content of the text column, pandas cannot display all of them at once by default. While doing EDA and data cleansing, we will need to see all of the columns. When the dataframe is presented, the code below allows us to see the whole set of columns in our data collection. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set option to display all columns\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=three2></a>\n",
    "\n",
    "#### 3.2 <strong>Check the \"Shape\" of the data-sets</strong></a>\n",
    "As demonstrated by the shape of both datasets, the data has been separated into two sets. The form also shows that the training data set has eight columns, but the test data set has seven . Our model is designed to forecast the column that is not present in the test set. We can look for that specific item by looking for the missing entity (Column) in the test data set. After looking at both datasets, the column may be identified as the SDG column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking the shape of the data sets\n",
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking the columns of the data set\n",
    "train_eda.columns, test_eda.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=four1></a>\n",
    "\n",
    "#### 3.3 <strong>Dataset summary</strong></a>\n",
    "\n",
    "It is important to identify the columns that have null entries as null values can affect the performance of our model. The \"isnull\" function shows the number of null values that are contained in each column of the dataset. This data set is relatively clean \n",
    "Pandas dataframe.info() function is used to get a concise summary of the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Summary(df):\n",
    "    i = df.info()\n",
    "    print (\"NUL Values\")\n",
    "    n = df.isna().sum()\n",
    "    return i,n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Summary(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"four\"></a>\n",
    "## 4. <strong>Data Preprocessing (Cleaning)</strong></a>\n",
    "<a class=\"anchor\" id=\"1.1\"></a>\n",
    "<a href=#cont>Back to Table of Contents</a>\n",
    "\n",
    "| ⚡ Description: Data Cleaning ⚡ |\n",
    "| :--------------------------- |\n",
    "| In this phase, we'll convert the data into a readable and desired format, as well as filter out the most relevant information.. |\n",
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=four1></a>\n",
    "### 4.1 <strong>Preprocessing the Dataset</strong></a>\n",
    "\n",
    "#### Due to the presence of a non numerical column in our datasets, some preprocessing processes must be performed, including:\n",
    "\n",
    "\n",
    "* letter casing :Converting all letters to upper case or lower case is called letter casing.\n",
    "\n",
    "* Tokenization :Tokenizing refers to the process of converting tweets to tokens. Words separated by spaces in a text are referred to as tokens.\n",
    "\n",
    "* Noise removal: Unwanted characters such as HTML tags, punctuation marks, special characters, white spaces, and so on are removed.\n",
    "\n",
    "* Stopwords should be removed because they don't contribute anything to the machine learning model. The nltk library can specify a list of stopwords, or it can be tailored to a particular company.\n",
    "\n",
    "* Lemmatization: the process of reducing a word's several forms to a single form, such as converting \"builds,\" \"building,\" or \"built\" to the lemma.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing links\n",
    "train['text'] = train['text'].str.replace('http\\S+|www.\\S+', '', case=False)\n",
    "train['text'] = train['text'].str.replace(r's*https?://S+(s+|$)', ' ',case=False).str.strip()\n",
    "test['text'] = test['text'].str.replace('http\\S+|www.\\S+', '', case=False)\n",
    "test['text'] = test['text'].str.replace(r's*https?://S+(s+|$)', ' ',case=False).str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    \"\"\"This function takes in pandas dataframe, removes URL hyperlinks, stopwords, punctuation noises,contractions and lemmatize the text.\"\"\"\n",
    "\n",
    "    tokenizer = TreebankWordTokenizer() \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stopwords_list = stopwords.words('english')\n",
    "    point_noise = string.punctuation + '0123456789'\n",
    "    \n",
    "    cleanText = re.sub(r'@[a-zA-Z0-9\\_\\w]+', '', text)#Remove @mentions\n",
    "    cleanText = re.sub(r'#[a-zA-Z0-9]+', '', cleanText) #Remove '#' symbols\n",
    "    cleanText = re.sub(r'RT', '', cleanText)#Remove RT from text\n",
    "    #Panding Contractions\n",
    "    # specific\n",
    "    cleanText = re.sub(r\"won\\'t\", \"will not\", cleanText)\n",
    "    cleanText = re.sub(r\"can\\'t\", \"can not\", cleanText)\n",
    "    cleanText = re.sub(r\"also\", \"\", cleanText)\n",
    "    #Panding Contractions\n",
    "    # general\n",
    "    cleanText = re.sub(r\"n\\'t\", \" not\", cleanText)\n",
    "    cleanText = re.sub(r\"\\'re\", \" are\", cleanText)\n",
    "    cleanText = re.sub(r\"\\'s\", \" is\", cleanText)\n",
    "    cleanText = re.sub(r\"\\'d\", \" would\", cleanText)\n",
    "    cleanText = re.sub(r\"\\'ll\", \" will\", cleanText)\n",
    "    cleanText = re.sub(r\"\\'t\", \" not\", cleanText)\n",
    "    cleanText = re.sub(r\"\\'ve\", \" have\", cleanText)\n",
    "    cleanText = re.sub(r\"\\'m\", \" am\", cleanText)\n",
    "    cleanText = ''.join([word for word in cleanText if word not in point_noise]) #Removing punctuations and numbers.\n",
    "    cleanText = cleanText.lower() #Lowering case\n",
    "    cleanText = \"\".join(word for word in cleanText if ord(word)<128) #Removing NonAscii\n",
    "    cleanText = tokenizer.tokenize(cleanText) #Coverting each words to tokens\n",
    "    cleanText = [lemmatizer.lemmatize(word) for word in cleanText if word not in stopwords_list] #Lemmatizing and removing stopwords\n",
    "    cleanText = [word for word in cleanText if len(word) >= 2]\n",
    "    # cleanText = ' '.join(cleanText)\n",
    "    #return cleanText\n",
    "    return cleanText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#applying the preprocess function\n",
    "train[\"text\"]=train[\"text\"].apply(preprocess)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[\"text\"]=test[\"text\"].apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"five\"></a>\n",
    "## 5. <strong>Exploratory Data Analysis (EDA)</strong></a>\n",
    "<a class=\"anchor\" id=\"1.1\"></a>\n",
    "<a href=#cont>Back to Table of Contents</a>\n",
    "\n",
    "---\n",
    "    \n",
    "| ⚡ Description: Exploratory data analysis ⚡ |\n",
    "| :--------------------------- |\n",
    "| We'll use a range of strategies in this part to maximize specific insights into our dataset, uncover underlying structure, extract relevant variables, find outliers and anomalies, test assumptions, and establish the optimum estimation parameters. In other words, we want to go deeper into our dataset in order to learn more about its behavior! |\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=four3></a>\n",
    "\n",
    "#### 5.1 <strong>Most Frequent Words</strong></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "cnt = Counter()\n",
    "for message in train['text'].values:\n",
    "    for word in message:\n",
    "        cnt[word] += 1\n",
    "        \n",
    "cnt.most_common(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=four3></a>\n",
    "\n",
    "#### 5.1 <strong>Visualisation</strong></a>\n",
    "---\n",
    "* Separate Datframes of Text for each SDG "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"See distribution of messages per SDG : \")\n",
    "count = train.groupby(\"sdg\").count()[\"text\"].reset_index().sort_values(by=\"text\", ascending=False)\n",
    "count.style.background_gradient(cmap=\"Purples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map the target variable name to their code for better understanding\n",
    "sdgLables = {1: \"No poverty\", 2: \"Zero Hunger\", 3: \"Good Health and well-being\", 4: \"Quality Education\", 5: \"Gender equality\", 6: \"Clean water and sanitation\", 7: \"Affordable and clean energy\", 9: \"Industry, Innovation and Infrustructure\", 8: \"Decent work and economic growth\",\n",
    "             10: \"Reduced Inequality\", 13: \"Climate Action\", 11: \"Sustainable cites and communities\", 12: \"Responsible consumption and production\", 14: \"life below water\", 15: \"Life on land\", 16: \"Peace , Justice and strong institutions\", 17: \"Partnership for the goals\"}\n",
    "train['SDG_Labels'] = train['sdg'].map(sdgLables)\n",
    "\n",
    "# Confirm the dataset\n",
    "train.head(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for a better visualisation let view by the lables\n",
    "print(\"See distribution of messages per sdg : \")\n",
    "count = train.groupby(\"SDG_Labels\").count()[\"text\"].reset_index().sort_values(by=\"text\", ascending=False)\n",
    "count.style.background_gradient(cmap=\"Purples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.bar(count, x='SDG_Labels', y=\"text\", color='text', title='Top SDG in the Text column',\n",
    "             template='plotly_white', labels={'ngram': 'Bigram', 'count': 'text'}).update_xaxes(categoryorder='total ascending')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.treemap(count, path=['SDG_Labels'],title='Treemap chart by Top SDG in the Text column',\n",
    "                 values='text', color= \"text\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations\n",
    "\n",
    "type here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=five4></a>\n",
    "\n",
    "#### 5.2 Investigative  Text Analysis\n",
    "From here, we'll delve further into our text, looking into each term and its frequency of occurrence individually.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = train['text']\n",
    "allwords = []\n",
    "for wordlist in words:\n",
    "    allwords += wordlist\n",
    "    \n",
    "mostcommon = FreqDist(allwords).most_common(10000)\n",
    "wordcloud = WordCloud(width=700, height=700, background_color=\n",
    "                      'black').generate(str(mostcommon))\n",
    "fig = plt.figure(figsize=(30,10), facecolor='white')\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis('off')\n",
    "plt.tight_layout(pad=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Checking class balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting the class distribution\n",
    "train.sdg.value_counts().plot(kind='bar')\n",
    "plt.ylabel('Frequency')\n",
    "plt.xlabel('sdg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the distribution plot above, we can see that the class distribution has wide variations and it is very imbalance. Although, most sdg support the gender equality which is represented by `5` while we have a fewer responsibe consumption and production which is represented by `12`.\n",
    "\n",
    "\n",
    "&emsp;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Dealing with data imbalance "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To deal with data imbalance and avoid bias predictions from the models we will build later on, we employed the resampling (Upsampling and Downsampling) method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['text'] = train.text.apply(lambda x: ' '.join([str(i) for i in x])) #code to change from list to string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['text'] = test.text.apply(lambda x: ' '.join([str(i) for i in x])) #code to change from list to string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_size = 4000 # setting an average or fixed size for each category in 'sdg'\n",
    "dd = train.sdg.value_counts() # getting category name and their size\n",
    "appended_data = [] # creating an empty list to append all category after resampling\n",
    "\n",
    "# Creating a for-loop to resample and append to a list\n",
    "for index, size in dd.items():\n",
    "    if size < class_size: # setting condition to check if it's downsampling or otherwise\n",
    "        temp_pd = resample(train[train['sdg']==index],\n",
    "                          replace=True, # sample with replacement\n",
    "                          n_samples=class_size, # match number in majority class\n",
    "                          random_state=27)\n",
    "    else:\n",
    "        temp_pd = resample(train[train['sdg']==index],\n",
    "                          replace=False, # sample with replacement (no need to duplicate observations)\n",
    "                          n_samples=class_size, # match number in minority class\n",
    "                          random_state=27)\n",
    "# Appending each category after resampling\n",
    "    appended_data.append(temp_pd)\n",
    "\n",
    "# Creating a new dataframe and viewing\n",
    "df_train_sampled = pd.concat(appended_data, axis=0)\n",
    "print('dual-sampling:')\n",
    "display(df_train_sampled.sample(1))\n",
    "print('Class with their new resampled sizes')\n",
    "# print(df_train_sampled.sdg.value_counts())\n",
    "\n",
    "# ploting the old and new graph frequency\n",
    "ax = train.sdg.value_counts().plot(kind='bar')\n",
    "df_train_sampled.sdg.value_counts().plot(kind='bar', title='Count (target)', \n",
    "                                               ax=ax,color='orange', alpha=0.5)\n",
    "plt.xticks(range(len(dd)),dd.index)\n",
    "plt.ylabel(\"# of observations\")\n",
    "plt.legend(['original','resampled'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41007dfb",
   "metadata": {},
   "source": [
    "<a id=\"seven\"></a>\n",
    "## 7. Data or Feature Engineering\n",
    "<a class=\"anchor\" id=\"1.1\"></a>\n",
    "<a href=#cont>Back to Table of Contents</a>\n",
    "\n",
    "---\n",
    "    \n",
    "| ⚡ Description: Data engineering ⚡ |\n",
    "| :--------------------------- |\n",
    "| In this section we would be: We will be looking at one of feature selection in text data namely the `tfidfVectorizer`. |\n",
    "\n",
    "---\n",
    "<a id='seven1'></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4330885e",
   "metadata": {},
   "source": [
    "### 7.1 Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ca56e7",
   "metadata": {},
   "source": [
    "At this point,we are using **CountVectorizer** function to convert the cleaned text to numerical entries in a matrix form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a4ae88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting each word in the dataset into features\n",
    "vectorizer = CountVectorizer(analyzer = \"word\", max_features = 8000) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d21402",
   "metadata": {},
   "source": [
    "we set the maximum feature to random number 8000 by discretion and to have a faster training of our model<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transforming the cleaned tweet to numerical entries in a matrix form\n",
    "reviews_vect = vectorizer.fit_transform(train['text'])\n",
    "\n",
    "# converting the features to numpy array for fast fitting of our dataset\n",
    "train_data_features = reviews_vect.toarray()\n",
    "\n",
    "# checking the shape to be sure of the size of the features\n",
    "print(f'Our new vectorized features have {train_data_features.shape[0]} rows and {train_data_features.shape[1]} feaures')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the word of the all tweet into features using 'vectorizer' object created earlier\n",
    "reviews_vect_test = vectorizer.transform(test['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting into a numpy array\n",
    "test_data_features = reviews_vect_test.toarray() # for fast fitting of our dataset\n",
    "test_data_features.shape # checking to be sure is the same size with our 'train' dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Double checking our dataframe was modified rightly\n",
    "test.sample(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"six\"></a>\n",
    "## 8. Classification Models\n",
    "<a class=\"anchor\" id=\"1.1\"></a>\n",
    "<a href=#cont>Back to Table of Contents</a>\n",
    "\n",
    "---\n",
    "    \n",
    "| ⚡ Description: Classification Models ⚡ |\n",
    "| :--------------------------- |\n",
    "| In this section, we'll look at different models, try to explain them, and learn about their benefits and drawbacks. |\n",
    "\n",
    "---\n",
    "\n",
    "*  LinearSVC(Support Vector Classifier)<a id='svc'></a>\n",
    "*  LGBMClassifier(Light Gradient Boosting Machine Classifier)<a id='LGBM'></a>\n",
    "*  Logistic Regression\n",
    "*  Stochastic\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1 Defining the independent (X) and dependent variables (y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting the target and features \n",
    "y = np.array(train['sdg'])\n",
    "X = train_data_features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 Spliting into Train and validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the train data set is divided into train and validation data set. The validation set size is 20% of the total records and it will be used in model evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the target ad features into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e2c478",
   "metadata": {},
   "source": [
    "Splitting of the dataset into train and validation dataset, so that we can train and also see the performance of the model on unseen dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8faa4002",
   "metadata": {},
   "source": [
    "### 8.3 Modeling building"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28fbb43",
   "metadata": {},
   "source": [
    "### 8.3.1 Defining function for building models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424b4dd5",
   "metadata": {},
   "source": [
    "The function implements a machine learning moded with the use of the train_test_split concept. The function calculates various metrics to check for underfitting or overfitting and most importantly the performance of the model being executed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdgLables = {v: k for k, v in sdgLables.items()}\n",
    "sdgLables = {'No poverty': 1, 'Zero Hunger': 2, 'Good Health and well-being': 3, 'Quality Education': 4, 'Gender equality': 5, 'Clean water and sanitation': 6, 'Affordable and clean energy': 7, 'Industry, Innovation and Infrustructure': 9, 'Decent work and economic growth': 8,\n",
    "             'Reduced Inequality': 10, 'Climate Action': 13, 'Sustainable cites and communities': 11, 'Responsible consumption and production': 12, 'life below water': 14, 'Life on land': 15}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8256e94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# careating a function to fit the model and calculate various metrics\n",
    "def modelling(model, X_train=X_train, X_test=X_test, y_train=y_train, y_test=y_test):\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    pred_train = model.predict(X_train)\n",
    "    pred_valid = model.predict(X_test)\n",
    "    \n",
    "# calculating the metrics\n",
    "    f1_train = f1_score(y_train, pred_train, average='weighted')\n",
    "    f1_test = f1_score(y_test, pred_valid,  average='weighted')\n",
    "    cm = confusion_matrix(y_test, pred_valid)\n",
    "    cr = classification_report(y_test, pred_valid, target_names=sdgLables, output_dict=True)\n",
    "    \n",
    "# creating a dataframe for 'confusion matrix' and 'classification report'\n",
    "    cm_df = pd.DataFrame(cm, index = sdgLables, columns = sdgLables)\n",
    "    cr_df = pd.DataFrame(cr)\n",
    "    \n",
    "    print('Confusion Matrix')\n",
    "    display(cm_df)\n",
    "    print('Classification Report')\n",
    "    display(cr_df)\n",
    "    \n",
    "# plotting the confusion matrix using heatmap\n",
    "    plt.figure(figsize=(10,5))\n",
    "    sns.heatmap(cm_df, annot=True)\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.ylabel('Actal Values')\n",
    "    plt.xlabel('Predicted Values')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f'Train accuracy(f1) is: {round(f1_train, 3)} and Validation accuracy(f1) is {round(f1_test, 3)}')\n",
    "    return cm_df, cr_df, cm # returning metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8df6776",
   "metadata": {},
   "source": [
    "### 8.3.2  Applying the modelling function to build models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adbe7cd1",
   "metadata": {},
   "source": [
    "#### a) Logistic Regression Classifier\n",
    "\n",
    "Logistics regression is a predictive analytics model that is used to describe data and explain the relationship between one dependent binary variable and one or more nominal, ordinal, interval or ratio-level independent variables.Sometimes, it can also be applied to a dependent variable that is multi-classed (ie have more than 2 variables). The model was adopted because it is a classification algorithm and suited our use case as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d2665f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# create logistic regression model instance\n",
    "lm = LogisticRegression(C=2, solver='liblinear')\n",
    "\n",
    "# fitting and predictiion by calling the 'modelling' fuction \n",
    "cm_df_logit, cr_df_logit, cm_logit = modelling(lm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c5f533",
   "metadata": {},
   "source": [
    "From the confusion matrix we can see that the model did well at predicting the `Gender equality`. Therefore, we have more correctly predicted `Gender equality` sentiment than others with an accuracy of `78%`\n",
    "\n",
    "* Generally the model is overfitting having an accuracy score of `99%` in training vs `71%` while predicting unseen dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionaries for the data we want to log on comet\n",
    "\n",
    "params = {\"random_state\": 10,\n",
    "          \"model_type\": \"lm\"\n",
    "          }\n",
    "metrics = {\"f1\": cr_df_logit.loc['f1-score', 'weighted avg'],\n",
    "           \"precision\": cr_df_logit.loc['precision', 'weighted avg'],\n",
    "           \"recall\": cr_df_logit.loc['recall', 'weighted avg']\n",
    "           }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log our parameters and results on comet\n",
    "experiment.log_parameters(params)\n",
    "experiment.log_metrics(metrics)\n",
    "experiment.log_confusion_matrix(matrix=cm_logit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b) Support Vector Machine Classifier\n",
    "\n",
    "A linear model for classification and regression issues is the SVM, or Support Vector Machine. It works well for many real-world issues and can solve both linear and non-linear problems. The SVM concept is straightforward: A line or a hyperplane that divides the data into classes is produced by the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#implementing SVC. Please note that this took 47min to run on an i5 12gig ram 1terabite laptop\n",
    "Uncomment if you want to run\n",
    "svc = SVC(kernel='linear')\n",
    "\n",
    "# fitting and predictiion by calling the 'modelling' fuction \n",
    "cm_df_svc, cr_df_svc, cm_svc = modelling(svc)\n",
    "\n",
    "# loging the confusion matric to comet\n",
    "#experiment.log_confusion_matrix(matrix=cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generally the model is overfitting having an accuracy score of approximately `100%` in training vs `70%` while predicting unseen dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d5c11c",
   "metadata": {},
   "source": [
    "### c)  Decision Tree classifier\n",
    "\n",
    "Decision Tree is a Supervised learning technique that can be used for both classification and Regression problems, but mostly it is preferred for solving Classification problems. It is a tree-structured classifier, where internal nodes represent the features of a dataset, branches represent the decision rules and each leaf node represents the outcome. In order to make prediction, the algorithm simply asks a question, and based on the answer (Yes/No), it further split the tree into subtrees till it get to the final outcome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2125078d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inintiate Decision tree\n",
    "dt=DecisionTreeClassifier(random_state=1)\n",
    "\n",
    "#initiate a BaggingClassifier called bc consisting of 50 trees\n",
    "bc=BaggingClassifier(base_estimator=dt, n_estimators=50, random_state=1, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_df_dt, cr_df_dt, cm_dt = modelling(dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcfcc86b",
   "metadata": {},
   "source": [
    "From our result, the model performed poorly on unseen data. With the train data, an accuracy score of `100%` was recorded but on the test data, we have an accuracy score of `57.4%` which depicts overfitting. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171fa99e",
   "metadata": {},
   "source": [
    "### Using pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb4cc1a",
   "metadata": {},
   "source": [
    "Moving forward to build the rest of our model, we employed techiques like `Pipeline` and `GridsearchCV`.\n",
    "\n",
    "**Pipeline**\n",
    "\n",
    "When it comes to orchestrating the flow of data into and output from a machine learning model, a machine learning pipeline comes in handy. It is known as an end-to-end structure that orchestrates the flow of data into and output from a machine learning model is known as a machine learning pipeline (or set of multiple models). It covers the input of the raw data, the features, the outputs, the machine learning model and model parameters, and the outputs of the predictions.\n",
    "\n",
    "Here, a machine learning pipeline is employed to easy feature extraction from our text data. The pipeline employed executes the following steps in chronological order to extract these features and make predictions. They include:\n",
    "\n",
    "* Count Vectorization: The CountVectorizer tool in Scikit-learn is used to turn a group of text documents into a vector of term/token counts. Additionally, it enables text data to be pre-processed before being turned into a vector form. Its capabilities turn it into a very versatile feature representation module for text.\n",
    "&emsp;\n",
    "\n",
    "\n",
    "* TF-ID Transformer: With TF-ID transformer, a count matrix is transformed into a normalised tf or tf-idf representation. To reduce the impact of tokens that appear often in a corpus, tf-idf is used in place of the raw frequencies of occurrence of a token in a specific document \n",
    "\n",
    "\n",
    "\n",
    "* Model: This represents the adopted Machine Learning model\n",
    "\n",
    "\n",
    "**GridsearchCV**\n",
    "\n",
    "GridSearchCV is the process of performing hyperparameter tuning in order to determine the optimal values for a given model. The performance of a model significantly depends on the value of hyperparameters and there is no way to know in advance the best values for hyperparameters. So ideally, we need to try all possible values to know the optimal values. That is why we employ GridSearchCV to automate the tuning of hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69abd5ff",
   "metadata": {},
   "source": [
    "### d) Naive Bayes classifier\n",
    "\n",
    "Mostly employed in Natural Language Processing, the Multinomial Naive Bayes algorithm is a probabilistic learning technique (NLP). The method, which guesses the tag of a text such as an email or newspaper article, is based on the Bayes theorem. For a given sample, it determines the probabilities of each tag, and then outputs the tag with the highest probability.\n",
    "\n",
    "The Naive Bayes classifier is a collection of many methods, all of which are based on the idea that each feature being classified is independent of every other feature. The existence or absence of one feature has no bearing on the other feature's existence or absence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a09478",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the target and features into train and test\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(train['text'], train.sdg, test_size=0.2, random_state=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd652a6",
   "metadata": {},
   "source": [
    "Here, we split our dataset again into `Train` and `Validation` set for the purpose of fitting the data into our pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b2712e",
   "metadata": {},
   "source": [
    "##### Building a pipeline to be more concise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ed0060",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipline with naive_bayes and using the 'clean_tweet' from dataset to fit the model\n",
    "text_clf = Pipeline([('vect', CountVectorizer()),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', MultinomialNB()),])\n",
    "# cross validation\n",
    "scores = cross_validate(text_clf, train['text'], train.sdg, return_train_score=True)\n",
    "pd.DataFrame(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3035c1",
   "metadata": {},
   "source": [
    "The model seem to be under performing and overfitting at the same time\n",
    "- Using GridsearchCV we can find the best hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e020cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'vect__ngram_range': [(1, 1), (1, 2), (2, 2)],\n",
    "              'clf__alpha': (1e-2, 1e-3),\n",
    "             'clf__fit_prior': (True, False)}\n",
    "nb_clf = GridSearchCV(text_clf, parameters, n_jobs=-1)\n",
    "nb_clf = nb_clf.fit(x_train, y_train)\n",
    "display('The best accuracy is:',nb_clf.best_score_)\n",
    "display(nb_clf.best_params_)\n",
    "\n",
    "# predictions\n",
    "predicts = nb_clf.predict(x_valid)\n",
    "cm = confusion_matrix(y_valid,  predicts)\n",
    "pd.DataFrame(cm, index = sdgLables, columns = sdgLables)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060906f8",
   "metadata": {},
   "source": [
    "Using our split dataset so we can train on seen data and validate our model with the unseen dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e71df992",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the trained model as a pickle file\n",
    "Pkl_Filename = \"naive_bayes_model.pkl\"  \n",
    "\n",
    "with open(Pkl_Filename, 'wb') as file:  \n",
    "    pickle.dump(nb_clf, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4162d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionaries for the data we want to log\n",
    "params = nb_clf.best_params_\n",
    "metrics = {\"f1\": nb_clf.best_score_}\n",
    "\n",
    "# Log our parameters and results\n",
    "experiment.log_parameters(params)\n",
    "experiment.log_metrics(metrics)\n",
    "experiment.log_confusion_matrix(matrix=cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66af18ef",
   "metadata": {},
   "source": [
    "### e) K-Neighbors Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d56520a6",
   "metadata": {},
   "source": [
    "Building a pipeline to be more concise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74962c8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# pipline with naive_bayes and using the 'clean_tweet' from dataset to fit the model\n",
    "knc_clf = Pipeline([('vect', CountVectorizer()),\n",
    "                    ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', KNeighborsClassifier()),])\n",
    "# cross validation\n",
    "scores = cross_validate(knc_clf, train['text'], train.sdg, return_train_score=True)\n",
    "pd.DataFrame(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f7e1c0",
   "metadata": {},
   "source": [
    "The model seem to be under performing and overfitting at the same time\n",
    "- Using GridsearchCV we can find the best hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97319e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'vect__ngram_range': [(1, 1), (1, 2)],\n",
    "              'clf__n_neighbors': (1, 31)}\n",
    "knn_clf = GridSearchCV(knc_clf, parameters, n_jobs=-1)\n",
    "knn_clf = knn_clf.fit(x_train, y_train)\n",
    "display(f'The best accuracy is: {knn_clf.best_score_}')\n",
    "display(knn_clf.best_params_)\n",
    "\n",
    "# predictions\n",
    "predicts = knn_clf.predict(x_valid)\n",
    "cm = confusion_matrix(y_valid,  predicts)\n",
    "pd.DataFrame(cm, index = sdgLables, columns = sdgLables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e71df992",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the trained model as a pickle file\n",
    "Pkl_Filename = \"KNeighborsC_model.pkl\"  \n",
    "\n",
    "with open(Pkl_Filename, 'wb') as file:  \n",
    "    pickle.dump(knn_clf, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionaries for the data we want to log\n",
    "\n",
    "params = knn_clf.best_params_\n",
    "metrics = {\"f1\": knn_clf.best_score_}\n",
    "\n",
    "# Log our parameters and results\n",
    "experiment.log_parameters(params)\n",
    "experiment.log_metrics(metrics)\n",
    "experiment.log_confusion_matrix(matrix=cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a19900",
   "metadata": {},
   "source": [
    "The above cells are used to log/commit our model to comet "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58622c4",
   "metadata": {},
   "source": [
    "<a id=\"seven\"></a>\n",
    "## 7. Model Performance\n",
    "<a class=\"anchor\" id=\"1.1\"></a>\n",
    "<a href=#cont>Back to Table of Contents</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8487a3c7",
   "metadata": {},
   "source": [
    "In order to evaluate how well our models have performed, we employed the following classification metrics:\n",
    "\n",
    "* **1. Accuracy**\n",
    "\n",
    "Accuracy simply measures how often the classifier correctly predicts. We can define accuracy as the ratio of the number of correct predictions and the total number of predictions.\n",
    "\n",
    "* **2. Precision**\n",
    "\n",
    "Precision explains how many of the correctly predicted cases actually turned out to be positive. Precision is useful in the cases where False Positive is a higher concern than False Negatives.\n",
    "\n",
    "* **3. Recall**\n",
    "\n",
    "Recall explains how many of the actual positive cases we were able to predict correctly with our model. It is a useful metric in cases where False Negative is of higher concern than False Positive.\n",
    "\n",
    "* **4. F1-Score**\n",
    "\n",
    "It gives a combined idea about Precision and Recall metrics.F1 Score is the harmonic mean of precision and recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d473cfbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8f918f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting with each model of choice\n",
    "lm_preds = lm.predict(X_test) # LogisticRegression\n",
    "nb_clf_preds = nb_clf.predict(x_valid) # Naive Bayes\n",
    "knn_clf_preds = knn_clf.predict(x_valid)# K-Neighbors Classifier\n",
    "svc_preds = svc.predict(X_test) #Linear SVC\n",
    "dt_preds = dt.predict(X_test) # Decision Tree Classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment this part and run if you runed the SVC up above \n",
    "# creating a map for our model and there prediction array of values\n",
    "metrics_dict = {\"LogisticRegression\":lm_preds, \"Naive Bayes\":nb_clf_preds, \"K-Neighbors Classifier\":knn_clf_preds,\n",
    "                \"Linear SVC\": svc_preds,\"Decision Tree Classifier\": dt_preds}\n",
    "metrics_list = [] #initiating a new list to store our metrics value temporarily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a map for our model and there prediction array of values\n",
    "metrics_dict = {\"LogisticRegression\":lm_preds, \"Naive Bayes\":nb_clf_preds, \"K-Neighbors Classifier\":knn_clf_preds,\n",
    "                \"Decision Tree Classifier\": dt_preds}\n",
    "metrics_list = [] #initiating a new list to store our metrics value temporarily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterating through the dictionary to calculate metrics for each model and make a dataframe off it\n",
    "for name, prediction in metrics_dict.items():\n",
    "    f1_score = metrics.f1_score(y_valid, prediction, average=\"weighted\")  \n",
    "    precision = metrics.precision_score(y_valid, prediction, average=\"weighted\")\n",
    "    recall = metrics.recall_score(y_valid, prediction, average=\"weighted\")\n",
    "    metrics_list.append([name, f1_score, precision, recall])\n",
    "perf_df = pd.DataFrame(metrics_list, columns=[\"Classifier\", \"F1_score\", \"Precision\", \"Recall\"])\n",
    "perf_df.set_index(\"Classifier\", inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd06df92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# overview of the dataframe created above\n",
    "perf_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9afaddc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting the metrics complied \n",
    "perf_df.plot(kind=\"bar\", figsize=(15,6));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce341772",
   "metadata": {},
   "source": [
    "All the Classification metrics point to `Naive Bayes` as the better Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d7f5f03",
   "metadata": {},
   "source": [
    "#### predicting our `test` DataSet with the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b93acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# logistic reggression\n",
    "# Using the fitted (train) model to predict the 'test' dataset\n",
    "pred_test = lm.predict(test_data_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b93acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Naive bayes is given me errors .fix it if you can\n",
    "# # Using the fitted (train) model to predict the 'test' dataset\n",
    "# pred_test = nb_clf.predict(test_data_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving test predictions to csv file\n",
    "output = pd.DataFrame({'id': test.id,\n",
    "                       'sdg': pred_test})\n",
    "output.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "320c1f05b41b6296d6cdeadbc8f37198b22e160db062b16d8b8cc9d95c25d782"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
